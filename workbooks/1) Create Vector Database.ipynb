{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da0b404e-bd56-4f04-b925-3a7c53a48ef8",
   "metadata": {},
   "source": [
    "# Create Vector Database\n",
    "\n",
    "First we will create the database of vectors and words. I don't believe for such a small project that we will need a SQLite databse, so we will simple save the text data as a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d25bf7ad-dc8f-43e1-96cd-c6c5367388b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\projects\\python\\RAG-Proof-Concept\\rag_env\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\projects\\python\\RAG-Proof-Concept\\rag_env\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import fitz  # PyMuPDF for PDF text extraction\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rank_bm25 import BM25Okapi\n",
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f3acdc-7a17-49ac-8703-da43896c81dd",
   "metadata": {},
   "source": [
    "Model we will use is a Hugging Face transformer model saved on my local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31e3f904-f68c-4759-bcdc-f849b942cdf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\projects\\python\\RAG-Proof-Concept\\rag_env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize Sentence-BERT model and BERT tokenizer\n",
    "model_name = \"../models/all-mpnet-base-v2\"\n",
    "model = SentenceTransformer(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', clean_up_tokenization_spaces=True)\n",
    "\n",
    "# FAISS setup for vector search\n",
    "embedding_dimension = model.get_sentence_embedding_dimension()  # Get the dimension size of the embeddings\n",
    "index = faiss.IndexFlatL2(embedding_dimension)  # L2 distance for similarity search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5875ee9-663d-4ac3-b20f-d50d9db43f10",
   "metadata": {},
   "source": [
    "Chunk up the text data so as not to max out the token limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84a9766a-0368-4e77-b118-516d7b16d356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text from PDFs using PyMuPDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file using PyMuPDF.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "\n",
    "    doc.close()\n",
    "    return text\n",
    "    \n",
    "def chunk_text(text, tokenizer, max_tokens=512):\n",
    "    \"\"\"Chunk the text based on sentences, ensuring no chunk exceeds the token limit.\"\"\"\n",
    "    # Split text into sentences\n",
    "    sentences = text.split('. ')  # You can adjust the sentence splitter to suit the document type\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_chunk_token_count = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Tokenize each sentence to get the number of tokens\n",
    "        sentence_tokens = tokenizer.tokenize(sentence)\n",
    "        sentence_token_count = len(sentence_tokens)\n",
    "        \n",
    "        # Check if adding this sentence would exceed the token limit\n",
    "        if current_chunk_token_count + sentence_token_count <= max_tokens:\n",
    "            current_chunk.append(sentence)\n",
    "            current_chunk_token_count += sentence_token_count\n",
    "        else:\n",
    "            # Save the current chunk and reset for the next chunk\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_chunk_token_count = sentence_token_count\n",
    "    \n",
    "    # Append the last chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    \n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b61b05-8ac8-4fb0-84c5-a96c2d761b5f",
   "metadata": {},
   "source": [
    "Create a function that will process the pdfs and store vectors values in the FAISS index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d68f4337-753e-4c83-94cf-cc35b09fe888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdfs_and_store_faiss(pdf_folder, pdf_files, tokenizer, model, faiss_index, text_chunk_df, bm25_corpus, doc_chunks):\n",
    "    \"\"\"Process all PDFs, tokenize, vectorize, and store embeddings in FAISS.\"\"\"\n",
    "    doc_ids = []  # Keep track of which document corresponds to which vectors\n",
    "    all_embeddings = []\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "        print(f\"Processing: {pdf_file}\")\n",
    "        \n",
    "        # Step 1: Extract text from the PDF\n",
    "        text = extract_text_from_pdf(pdf_path)\n",
    "        \n",
    "        # Step 2: Chunk text into smaller chunks based on sentences and 512 token limit\n",
    "        chunks = chunk_text(text, tokenizer, max_tokens=512)\n",
    "        \n",
    "        # Step 3: Generate embeddings for each chunk\n",
    "        embeddings = model.encode(chunks, convert_to_tensor=True)\n",
    "        embeddings = embeddings.cpu().numpy()  # Convert to numpy for FAISS\n",
    "        \n",
    "        # Step 4: Add embeddings to FAISS index\n",
    "        faiss_index.add(embeddings)\n",
    "        \n",
    "        # Keep track of document IDs and embedding count for reference\n",
    "        doc_ids.extend([pdf_file] * len(embeddings))\n",
    "        all_embeddings.append(embeddings)\n",
    "        \n",
    "        # Append the text chunks and file name to the DataFrame (for FAISS and final retrieval)\n",
    "        for chunk in chunks:\n",
    "            row_to_add = pd.DataFrame({\"chunk\": [chunk], \"file\": [pdf_file]})\n",
    "            text_chunk_df = pd.concat([text_chunk_df, row_to_add], ignore_index=True)\n",
    "        \n",
    "        # BM25 setup: tokenizing chunks for keyword search\n",
    "        for chunk in chunks:\n",
    "            bm25_corpus.append(chunk.split())  # Tokenized for BM25\n",
    "            doc_chunks.append(pdf_file)  # Track which document the chunk belongs to\n",
    "    \n",
    "    return doc_ids, all_embeddings, text_chunk_df, bm25_corpus, doc_chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9c74ba-2f9a-4d2b-a003-6ad3e96840d6",
   "metadata": {},
   "source": [
    "Process the PDFs by running the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3fc81c7-4406-4a65-a65e-de2467491e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: LSE_AZN_2022.pdf\n",
      "Processing: LSE_BP_2022.pdf\n",
      "Processing: LSE_ULVR_2022.pdf\n",
      "Processing: NASDAQ_AAPL_2022.pdf\n",
      "Processing: NASDAQ_AMZN_2022.pdf\n",
      "Processing: NASDAQ_INTC_2022.pdf\n",
      "Processing: NASDAQ_MSFT_2022.pdf\n",
      "Processing: NASDAQ_TSLA_2022.pdf\n",
      "Processing: NYSE_BA_2022.pdf\n",
      "Processing: NYSE_CVX_2022.pdf\n",
      "Processing: NYSE_GS_2022.pdf\n",
      "Processing: NYSE_HSBC_2022.pdf\n",
      "Processing: NYSE_JNJ_2022.pdf\n",
      "Processing: NYSE_JPM_2022.pdf\n",
      "Processing: NYSE_K_2022.pdf\n",
      "Processing: NYSE_MANU_2022.pdf\n",
      "Processing: NYSE_PFE_2022.pdf\n",
      "Processing: NYSE_V_2022.pdf\n",
      "Processing: NYSE_WK_2022.pdf\n",
      "Processing: NYSE_WMT_2022.pdf\n",
      "Processing: NYSE_XOM_2021.pdf\n",
      "Processing: OTC_NSRGY_2022.pdf\n"
     ]
    }
   ],
   "source": [
    "# Initialize required variables\n",
    "pdf_folder = '../data/raw'\n",
    "pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith('.pdf')]\n",
    "text_chunk_df = pd.DataFrame(columns=[\"chunk\", \"file\"])  # DataFrame to store text chunks and file names\n",
    "bm25_corpus = []  # To store all chunks for BM25\n",
    "doc_chunks = []  # To store chunks for BM25 document references\n",
    "\n",
    "# Run the script to process PDFs\n",
    "doc_ids, all_embeddings, text_chunk_df, bm25_corpus, doc_chunks = process_pdfs_and_store_faiss(\n",
    "    pdf_folder, pdf_files, tokenizer, model, index, text_chunk_df, bm25_corpus, doc_chunks\n",
    ")\n",
    "\n",
    "# Save the FAISS index and metadata for future use\n",
    "faiss.write_index(index, '../data/processed/financial_reports_faiss.index')\n",
    "\n",
    "\n",
    "# Save text_chunk_df as a CSV for future querying\n",
    "text_chunk_df.to_csv('../data/processed/text_chunk_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1def130f-a2f7-4955-b311-c3644ffac338",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "rag_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
